{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Document Code Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_context(\"notebook\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_NN_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212770 entries, 0 to 212769\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Unnamed: 0      212770 non-null  int64  \n",
      " 1   station_id      212770 non-null  int64  \n",
      " 2   hour            212749 non-null  object \n",
      " 3   departures      212770 non-null  float64\n",
      " 4   arrivals        212770 non-null  float64\n",
      " 5   hour_of_day     212770 non-null  int64  \n",
      " 6   day_of_week     212770 non-null  int64  \n",
      " 7   is_weekend      212770 non-null  int64  \n",
      " 8   hour_sin        212770 non-null  float64\n",
      " 9   hour_cos        212770 non-null  float64\n",
      " 10  lat             212770 non-null  float64\n",
      " 11  lon             212770 non-null  float64\n",
      " 12  date            212749 non-null  object \n",
      " 13  temperature_2m  212749 non-null  float64\n",
      " 14  precipitation   212749 non-null  float64\n",
      " 15  rain            212749 non-null  float64\n",
      " 16  snowfall        212749 non-null  float64\n",
      " 17  snow_depth      212749 non-null  float64\n",
      " 18  weather_code    212749 non-null  float64\n",
      " 19  wind_speed_10m  212749 non-null  float64\n",
      "dtypes: float64(13), int64(5), object(2)\n",
      "memory usage: 32.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>departures</th>\n",
       "      <th>arrivals</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>rain</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>2016-03-31 03:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>9.659258e-01</td>\n",
       "      <td>40.715422</td>\n",
       "      <td>-74.011220</td>\n",
       "      <td>2016-03-31 03:00:00+00:00</td>\n",
       "      <td>5.9335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.592731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>2017-02-23 11:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>40.714740</td>\n",
       "      <td>-74.009106</td>\n",
       "      <td>2017-02-23 11:00:00+00:00</td>\n",
       "      <td>3.7225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.489992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>2017-02-24 11:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>40.714740</td>\n",
       "      <td>-74.009106</td>\n",
       "      <td>2017-02-24 11:00:00+00:00</td>\n",
       "      <td>9.8225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.511088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>2017-02-27 11:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>40.714740</td>\n",
       "      <td>-74.009106</td>\n",
       "      <td>2017-02-27 11:00:00+00:00</td>\n",
       "      <td>-0.7775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.792404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>2017-02-28 11:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>40.714740</td>\n",
       "      <td>-74.009106</td>\n",
       "      <td>2017-02-28 11:00:00+00:00</td>\n",
       "      <td>4.0725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.620839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212765</th>\n",
       "      <td>212765</td>\n",
       "      <td>3426</td>\n",
       "      <td>2017-02-24 15:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>40.709651</td>\n",
       "      <td>-74.068601</td>\n",
       "      <td>2017-02-24 15:00:00+00:00</td>\n",
       "      <td>16.3725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.378250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212766</th>\n",
       "      <td>212766</td>\n",
       "      <td>3426</td>\n",
       "      <td>2017-03-05 17:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>40.709651</td>\n",
       "      <td>-74.068601</td>\n",
       "      <td>2017-03-05 17:00:00+00:00</td>\n",
       "      <td>-1.1275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.513195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212767</th>\n",
       "      <td>212767</td>\n",
       "      <td>3426</td>\n",
       "      <td>2017-03-10 14:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>40.709651</td>\n",
       "      <td>-74.068601</td>\n",
       "      <td>2017-03-10 14:00:00+00:00</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17.555307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212768</th>\n",
       "      <td>212768</td>\n",
       "      <td>3426</td>\n",
       "      <td>2017-03-31 13:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>40.709651</td>\n",
       "      <td>-74.068601</td>\n",
       "      <td>2017-03-31 13:00:00+00:00</td>\n",
       "      <td>3.4725</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.0</td>\n",
       "      <td>11.367109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212769</th>\n",
       "      <td>212769</td>\n",
       "      <td>3442</td>\n",
       "      <td>2017-03-29 12:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2017-03-29 12:00:00+00:00</td>\n",
       "      <td>6.1725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.440001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212770 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  station_id                       hour  departures  \\\n",
       "0                0         147  2016-03-31 03:00:00+00:00         0.0   \n",
       "1                1         152  2017-02-23 11:00:00+00:00         0.0   \n",
       "2                2         152  2017-02-24 11:00:00+00:00         0.0   \n",
       "3                3         152  2017-02-27 11:00:00+00:00         0.0   \n",
       "4                4         152  2017-02-28 11:00:00+00:00         0.0   \n",
       "...            ...         ...                        ...         ...   \n",
       "212765      212765        3426  2017-02-24 15:00:00+00:00         0.0   \n",
       "212766      212766        3426  2017-03-05 17:00:00+00:00         0.0   \n",
       "212767      212767        3426  2017-03-10 14:00:00+00:00         0.0   \n",
       "212768      212768        3426  2017-03-31 13:00:00+00:00         0.0   \n",
       "212769      212769        3442  2017-03-29 12:00:00+00:00         0.0   \n",
       "\n",
       "        arrivals  hour_of_day  day_of_week  is_weekend      hour_sin  \\\n",
       "0            2.0           23            2           0 -2.588190e-01   \n",
       "1            2.0            6            3           0  1.000000e+00   \n",
       "2            2.0            6            4           0  1.000000e+00   \n",
       "3            2.0            6            0           0  1.000000e+00   \n",
       "4            2.0            6            1           0  1.000000e+00   \n",
       "...          ...          ...          ...         ...           ...   \n",
       "212765       2.0           10            4           0  5.000000e-01   \n",
       "212766       4.0           12            6           1  1.224647e-16   \n",
       "212767       4.0            9            4           0  7.071068e-01   \n",
       "212768       4.0            9            4           0  7.071068e-01   \n",
       "212769       4.0            8            2           0  8.660254e-01   \n",
       "\n",
       "            hour_cos        lat        lon                       date  \\\n",
       "0       9.659258e-01  40.715422 -74.011220  2016-03-31 03:00:00+00:00   \n",
       "1       6.123234e-17  40.714740 -74.009106  2017-02-23 11:00:00+00:00   \n",
       "2       6.123234e-17  40.714740 -74.009106  2017-02-24 11:00:00+00:00   \n",
       "3       6.123234e-17  40.714740 -74.009106  2017-02-27 11:00:00+00:00   \n",
       "4       6.123234e-17  40.714740 -74.009106  2017-02-28 11:00:00+00:00   \n",
       "...              ...        ...        ...                        ...   \n",
       "212765 -8.660254e-01  40.709651 -74.068601  2017-02-24 15:00:00+00:00   \n",
       "212766 -1.000000e+00  40.709651 -74.068601  2017-03-05 17:00:00+00:00   \n",
       "212767 -7.071068e-01  40.709651 -74.068601  2017-03-10 14:00:00+00:00   \n",
       "212768 -7.071068e-01  40.709651 -74.068601  2017-03-31 13:00:00+00:00   \n",
       "212769 -5.000000e-01   0.000000   0.000000  2017-03-29 12:00:00+00:00   \n",
       "\n",
       "        temperature_2m  precipitation  rain  snowfall  snow_depth  \\\n",
       "0               5.9335            0.0   0.0      0.00        0.00   \n",
       "1               3.7225            0.0   0.0      0.00        0.00   \n",
       "2               9.8225            0.0   0.0      0.00        0.00   \n",
       "3              -0.7775            0.0   0.0      0.00        0.00   \n",
       "4               4.0725            0.0   0.0      0.00        0.00   \n",
       "...                ...            ...   ...       ...         ...   \n",
       "212765         16.3725            0.0   0.0      0.00        0.00   \n",
       "212766         -1.1275            0.0   0.0      0.00        0.00   \n",
       "212767          0.7725            1.4   0.0      0.98        0.01   \n",
       "212768          3.4725            0.8   0.8      0.00        0.00   \n",
       "212769          6.1725            0.0   0.0      0.00        0.00   \n",
       "\n",
       "        weather_code  wind_speed_10m  \n",
       "0                1.0       19.592731  \n",
       "1                0.0        6.489992  \n",
       "2                3.0        9.511088  \n",
       "3                0.0       11.792404  \n",
       "4                3.0        2.620839  \n",
       "...              ...             ...  \n",
       "212765           2.0       13.378250  \n",
       "212766           0.0       19.513195  \n",
       "212767          75.0       17.555307  \n",
       "212768          53.0       11.367109  \n",
       "212769           0.0       10.440001  \n",
       "\n",
       "[212770 rows x 20 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Initial features (before lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"arrivals\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"temperature_2m\",\n",
    "    \"precipitation\",\n",
    "    \"rain\",\n",
    "    \"snowfall\",\n",
    "    \"snow_depth\",\n",
    "    \"weather_code\",\n",
    "    \"wind_speed_10m\",\n",
    "]\n",
    "\n",
    "TARGET = \"departures\"\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "df[\"hour\"] = pd.to_datetime(df[\"hour\"])\n",
    "df = df.sort_values(\"hour\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating grouped dataset with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = df[\n",
    "    [\"station_id\", \"hour\"] + FEATURES + [TARGET]\n",
    "].dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure non-negative departures (important for log1p)\n",
    "df_nn[\"departures\"] = df_nn[\"departures\"].clip(lower=0)\n",
    "\n",
    "# Sort for time-safe lagging\n",
    "df_nn = df_nn.sort_values([\"station_id\", \"hour\"])\n",
    "\n",
    "# Lagged features\n",
    "df_nn[\"lag_1\"]   = df_nn.groupby(\"station_id\")[\"departures\"].shift(1)\n",
    "df_nn[\"lag_24\"]  = df_nn.groupby(\"station_id\")[\"departures\"].shift(24)\n",
    "df_nn[\"lag_168\"] = df_nn.groupby(\"station_id\")[\"departures\"].shift(168)\n",
    "\n",
    "df_nn[\"roll_24\"] = (\n",
    "    df_nn.groupby(\"station_id\")[\"departures\"]\n",
    "    .shift(1)\n",
    "    .rolling(24)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# ðŸš¨ THIS LINE IS MANDATORY ðŸš¨\n",
    "df_nn = df_nn.dropna().reset_index(drop=True)\n",
    "\n",
    "# Log-transform AFTER cleaning\n",
    "df_nn[\"y\"] = np.log1p(df_nn[\"departures\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"arrivals\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"hour_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"temperature_2m\",\n",
    "    \"precipitation\",\n",
    "    \"rain\",\n",
    "    \"snowfall\",\n",
    "    \"snow_depth\",\n",
    "    \"weather_code\",\n",
    "    \"wind_speed_10m\",\n",
    "    # lagged demand features\n",
    "    \"lag_1\",\n",
    "    \"lag_24\",\n",
    "    \"lag_168\",\n",
    "    \"roll_24\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating mapping bettwen stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping\n",
    "station_ids = df_nn[\"station_id\"].unique()\n",
    "station_to_idx = {sid: i for i, sid in enumerate(station_ids)}\n",
    "\n",
    "# Apply mapping\n",
    "df_nn[\"station_idx\"] = df_nn[\"station_id\"].map(station_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transfomring Predictor column to avoid heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn[\"y\"] = np.log1p(df_nn[TARGET])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing time based split since we are working with time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(df_nn) * 0.8)\n",
    "\n",
    "train_df = df_nn.iloc[:split_idx]\n",
    "val_df   = df_nn.iloc[split_idx:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df[FEATURES].values)\n",
    "X_val   = scaler.transform(val_df[FEATURES].values)\n",
    "\n",
    "y_train = train_df[\"y\"].values.reshape(-1, 1)\n",
    "y_val   = val_df[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "station_train = train_df[\"station_idx\"].values\n",
    "station_val   = val_df[\"station_idx\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandDataset(Dataset):\n",
    "    def __init__(self, X, station_id, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.station_id = torch.tensor(station_id, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.station_id[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DemandDataset(X_train, station_train, y_train)\n",
    "val_ds   = DemandDataset(X_val, station_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandNN(nn.Module):\n",
    "    def __init__(self, num_stations, num_features, emb_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.station_emb = nn.Embedding(num_stations, emb_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features + emb_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, station_id):\n",
    "        emb = self.station_emb(station_id)\n",
    "        x = torch.cat([x, emb], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_stations = df_nn[\"station_id\"].nunique()\n",
    "num_features = len(FEATURES)\n",
    "\n",
    "model = DemandNN(num_stations, num_features).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X, station_id, y in loader:\n",
    "        X = X.to(device)\n",
    "        station_id = station_id.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X, station_id)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(X)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, station_id, y in loader:\n",
    "            X = X.to(device)\n",
    "            station_id = station_id.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(X, station_id)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * len(X)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.2938 | Val Loss: 0.2440\n",
      "Epoch 02 | Train Loss: 0.2058 | Val Loss: 0.1984\n",
      "Epoch 03 | Train Loss: 0.1838 | Val Loss: 0.1895\n",
      "Epoch 04 | Train Loss: 0.1785 | Val Loss: 0.1875\n",
      "Epoch 05 | Train Loss: 0.1761 | Val Loss: 0.1870\n",
      "Epoch 06 | Train Loss: 0.1742 | Val Loss: 0.1898\n",
      "Epoch 07 | Train Loss: 0.1731 | Val Loss: 0.1888\n",
      "Epoch 08 | Train Loss: 0.1720 | Val Loss: 0.1917\n",
      "Epoch 09 | Train Loss: 0.1711 | Val Loss: 0.1905\n",
      "Epoch 10 | Train Loss: 0.1708 | Val Loss: 0.1992\n",
      "Epoch 11 | Train Loss: 0.1702 | Val Loss: 0.1967\n",
      "Epoch 12 | Train Loss: 0.1697 | Val Loss: 0.1935\n",
      "Epoch 13 | Train Loss: 0.1693 | Val Loss: 0.1928\n",
      "Epoch 14 | Train Loss: 0.1688 | Val Loss: 0.1948\n",
      "Epoch 15 | Train Loss: 0.1685 | Val Loss: 0.1936\n",
      "Epoch 16 | Train Loss: 0.1680 | Val Loss: 0.1946\n",
      "Epoch 17 | Train Loss: 0.1677 | Val Loss: 0.1971\n",
      "Epoch 18 | Train Loss: 0.1677 | Val Loss: 0.1946\n",
      "Epoch 19 | Train Loss: 0.1675 | Val Loss: 0.1986\n",
      "Epoch 20 | Train Loss: 0.1670 | Val Loss: 0.1948\n",
      "Epoch 21 | Train Loss: 0.1670 | Val Loss: 0.1946\n",
      "Epoch 22 | Train Loss: 0.1664 | Val Loss: 0.1929\n",
      "Epoch 23 | Train Loss: 0.1663 | Val Loss: 0.1938\n",
      "Epoch 24 | Train Loss: 0.1657 | Val Loss: 0.1922\n",
      "Epoch 25 | Train Loss: 0.1658 | Val Loss: 0.1957\n",
      "Epoch 26 | Train Loss: 0.1655 | Val Loss: 0.1922\n",
      "Epoch 27 | Train Loss: 0.1652 | Val Loss: 0.1977\n",
      "Epoch 28 | Train Loss: 0.1651 | Val Loss: 0.1927\n",
      "Epoch 29 | Train Loss: 0.1648 | Val Loss: 0.1947\n",
      "Epoch 30 | Train Loss: 0.1646 | Val Loss: 0.1960\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss = eval_epoch(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a set of predictions to ensure it can be swtich back in interms of number of bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21395354, 0.08630245, 2.6327817 , 2.978732  , 3.0221467 ,\n",
       "       3.130514  , 0.9277921 , 1.1133342 , 1.2486101 , 2.6773984 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_sample = torch.tensor(X_val[:10], dtype=torch.float32).to(device)\n",
    "    s_sample = torch.tensor(station_val[:10], dtype=torch.long).to(device)\n",
    "\n",
    "    log_preds = model(X_sample, s_sample).cpu().numpy().flatten()\n",
    "    preds = np.expm1(log_preds)\n",
    "\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_log = []\n",
    "y_val_true_log = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, station_id, y in val_loader:\n",
    "        X = X.to(device)\n",
    "        station_id = station_id.to(device)\n",
    "\n",
    "        preds = model(X, station_id)\n",
    "        y_val_pred_log.append(preds.cpu().numpy())\n",
    "        y_val_true_log.append(y.numpy())\n",
    "\n",
    "y_val_pred_log = np.vstack(y_val_pred_log).flatten()\n",
    "y_val_true_log = np.vstack(y_val_true_log).flatten()\n",
    "\n",
    "# Back to real units\n",
    "y_val_pred = np.expm1(y_val_pred_log)\n",
    "y_val_true = np.expm1(y_val_true_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining evaluation metrics for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv_splits(df, n_splits=5, min_train_frac=0.5):\n",
    "    \"\"\"\n",
    "    Generator yielding (train_idx, val_idx) for rolling-origin CV\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * min_train_frac)\n",
    "    step = (n - train_end) // n_splits\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        train_idx = np.arange(0, train_end + i * step)\n",
    "        val_idx = np.arange(train_end + i * step,\n",
    "                             min(train_end + (i + 1) * step, n))\n",
    "        yield train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    mae = np.mean(np.abs(y_pred - y_true))\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "    mask = y_true > 1\n",
    "    mape = np.mean(\n",
    "        np.abs(y_pred[mask] - y_true[mask]) / y_true[mask]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE\": mape\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running cross validation on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(time_series_cv_splits(df_nn)):\n",
    "\n",
    "    train_df = df_nn.iloc[train_idx]\n",
    "    val_df   = df_nn.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_df[FEATURES].values)\n",
    "    X_val   = scaler.transform(val_df[FEATURES].values)\n",
    "\n",
    "    y_train = train_df[\"y\"].values.reshape(-1, 1)\n",
    "    y_val   = val_df[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "    s_train = train_df[\"station_idx\"].values\n",
    "    s_val   = val_df[\"station_idx\"].values\n",
    "\n",
    "    train_ds = DemandDataset(X_train, s_train, y_train)\n",
    "    val_ds   = DemandDataset(X_val, s_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = DemandNN(\n",
    "        num_stations=len(station_to_idx),\n",
    "        num_features=len(FEATURES),\n",
    "        emb_dim=8\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "    # Train briefly (early stopping in practice)\n",
    "    for _ in range(10):\n",
    "        train_epoch(model, train_loader)\n",
    "\n",
    "    # Predict on validation\n",
    "    model.eval()\n",
    "    preds_log = []\n",
    "    true_log = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, s, y in val_loader:\n",
    "            X, s = X.to(device), s.to(device)\n",
    "            preds_log.append(model(X, s).cpu().numpy())\n",
    "            true_log.append(y.numpy())\n",
    "\n",
    "    preds = np.expm1(np.vstack(preds_log).flatten())\n",
    "    true  = np.expm1(np.vstack(true_log).flatten())\n",
    "\n",
    "    metrics = compute_metrics(true, preds)\n",
    "    metrics[\"fold\"] = fold\n",
    "    cv_results.append(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting evvaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.9143524169921875,\n",
       " 'RMSE': 3.246798038482666,\n",
       " 'MAPE': 0.5097813606262207,\n",
       " 'fold': 2.0}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "cv_summary = cv_df.mean().to_dict()\n",
    "cv_summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
